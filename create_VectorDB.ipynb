{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Creating the Vector Database with ChromaDB and Hugging Face Embeddings\n",
    "**Introduction:**  \n",
    "In this part, we will create a vector database using Chroma DB to store embeddings generated by Hugging Face's embedding models. This vector database will serve as the foundation for the retrieval component of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All packages are in requirements.txt\n",
    "\n",
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 350\n",
    "CHUNK_OVERLAP = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download an example PDF from arXiv\n",
    "For this RAG example we are using the Language Models are Few-Shot Learners paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Models are Few-Shot Learners\n",
      "http://arxiv.org/abs/2005.14165v4\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "search = arxiv.Search(id_list=['2005.14165'])\n",
    "\n",
    "paper = next(arxiv.Client().results(search))\n",
    "path = paper.download_pdf() \n",
    "print(paper.title)\n",
    "\n",
    "print(paper.entry_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert the PDF to LlamaIndex Documents\n",
    "For this example we will be using the Document format.\n",
    "This allows us to include the page_content and pass our metadata which is uses for citing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages 75\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(path)\n",
    "doc = []\n",
    "for idx, page in enumerate(reader.pages):\n",
    "    doc.append(Document(text=page.extract_text(),\n",
    "                        metadata={'source': f'{paper.title}', 'page': f'{idx+1}', 'link':f'{paper.entry_id}'},\n",
    "                        excluded_llm_metadata_keys=['link'],\n",
    "                        excluded_embed_metadata_keys=['source', 'page', 'link']\n",
    "))\n",
    "\n",
    "print(f'Number of pages {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Convert Documents into LlamaIndex Nodes\n",
    "We split our documents into 'chunks' to be embedded.  \n",
    "Each chunk is what LlamaIndex calls a **Node**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed the 75 pages into 249 nodes\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(include_metadata = True, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(doc)\n",
    "\n",
    "print(f'Parsed the {len(doc)} pages into {len(nodes)} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: Language Models are Few-Shot Learners\n",
      "page: 16\n",
      "\n",
      "Setting Winograd Winogrande (XL)\n",
      "Fine-tuned SOTA 90.1a84.6b\n",
      "GPT-3 Zero-Shot 88.3* 70.2\n",
      "GPT-3 One-Shot 89.7* 73.2\n",
      "GPT-3 Few-Shot 88.6* 77.7\n",
      "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n",
      "4 for details on potential contamination of the Winograd test set.a[SBBC19]b[LYN+20]\n",
      "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\n",
      "Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B\n",
      "is competitive with a ﬁne-tuned RoBERTA-large.\n",
      "each translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\n",
      "GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior\n",
      "unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the\n",
      "three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into\n",
      "English but underperforms when translating in the other direction.\n"
     ]
    }
   ],
   "source": [
    "# This prints what the LLM sees\n",
    "print (nodes[50].get_content (metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Winograd Winogrande (XL)\n",
      "Fine-tuned SOTA 90.1a84.6b\n",
      "GPT-3 Zero-Shot 88.3* 70.2\n",
      "GPT-3 One-Shot 89.7* 73.2\n",
      "GPT-3 Few-Shot 88.6* 77.7\n",
      "Table 3.5: Results on the WSC273 version of Winograd schemas and the adversarial Winogrande dataset. See Section\n",
      "4 for details on potential contamination of the Winograd test set.a[SBBC19]b[LYN+20]\n",
      "Figure 3.5: Zero-, one-, and few-shot performance on the adversarial Winogrande dataset as model capacity scales.\n",
      "Scaling is relatively smooth with the gains to few-shot learning increasing with model size, and few-shot GPT-3 175B\n",
      "is competitive with a ﬁne-tuned RoBERTA-large.\n",
      "each translation task improves performance by over 7 BLEU and nears competitive performance with prior work.\n",
      "GPT-3 in the full few-shot setting further improves another 4 BLEU resulting in similar average performance to prior\n",
      "unsupervised NMT work. GPT-3 has a noticeable skew in its performance depending on language direction. For the\n",
      "three input languages studied, GPT-3 signiﬁcantly outperforms prior unsupervised NMT work when translating into\n",
      "English but underperforms when translating in the other direction.\n"
     ]
    }
   ],
   "source": [
    "# This prints what the embedding sees, you can see excluding source and page worked\n",
    "print (nodes[50].get_content (metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Device:  \n",
    "If you are using a Mac or an Nvidia GPU and installed PyTorch correctly the below will use the correct device  \n",
    "Otherwise it will default to using the CPU\n",
    "\n",
    "For details on how to install PyTorch for CUDA see the [Get Started page](https://pytorch.org/get-started/locally/)  \n",
    "If you are not using CUDA with an Nvidia GPU you can uncomment the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch for Mac or Windows PC without Nvidia GPU \n",
    "# !pip install torch torchvision torchaudio \n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# Detect hardware acceleration device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Embedding Model:**  \n",
    "A good place to start when choosing and embedding model is the [MTEB English Leaderboard](https://huggingface.co/BAAI/bge-small-en)\n",
    "\n",
    "At time of writing, the [BAAI/bge-small-en-v1.5'model](https://huggingface.co/spaces/mteb/leaderboard) is the best small model according to the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model_name = 'BAAI/bge-small-en-v1.5'\n",
    "# Import embedding model from HuggingFace \n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embed_model_name,\n",
    "    device = device,\n",
    "    normalize='True', \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and store the Vector DB\n",
    "* This will use the bge-small-en embeddings model to embed our chunked text into vectors\n",
    "* Then save those vectors into a ChromaDB named \"RAG_VectorDB\" \n",
    "\n",
    "**Note**: If a DB with that name already exists, it will append, otherwise it creates it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "623d66bbb4934862b4312387e4be7fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "db = chromadb.PersistentClient(path='./RAG_VectorDB')\n",
    "\n",
    "collection_metadata = {\n",
    "    'embedding_used':embed_model_name,\n",
    "    'Included Papers':paper.title}\n",
    "chroma_collection = db.get_or_create_collection('arxiv_PDF_DB', metadata=collection_metadata)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model,\n",
    "                                                llm = None, # We will set the LLM when we open the DB\n",
    "                                                chunk_size=CHUNK_SIZE,\n",
    "                                                chunk_overlap=CHUNK_OVERLAP\n",
    "                                                )\n",
    "\n",
    "vector_store_index = VectorStoreIndex(nodes=nodes,\n",
    "                                    storage_context=storage_context, \n",
    "                                    service_context=service_context,\n",
    "                                    show_progress=True)\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
