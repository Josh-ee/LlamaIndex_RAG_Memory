{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Creating the Vector Database with ChromaDB and Hugging Face Embeddings\n",
    "**Introduction:**  \n",
    "In this part, we will create a vector database using Chroma DB to store embeddings generated by Hugging Face's embedding models. This vector database will serve as the foundation for the retrieval component of our RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install chromadb\n",
    "# !pip install arxiv\n",
    "# !pip install PyPDF2\n",
    "# !pip install llama-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE = 800\n",
    "CHUNK_OVERLAP = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Download an example PDF from arXiv\n",
    "For this RAG example we are using the Language Models are Few-Shot Learners paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language Models are Few-Shot Learners\n"
     ]
    }
   ],
   "source": [
    "client = arxiv.Client()\n",
    "search = arxiv.Search(id_list=['2005.14165'])\n",
    "\n",
    "paper = next(arxiv.Client().results(search))\n",
    "path = paper.download_pdf() \n",
    "print(paper.title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Convert the PDF to LlamaIndex Documents\n",
    "For this example we will be using the Document format.\n",
    "This allows us to include the page_content and pass our metadata which is uses for citing sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages 75\n"
     ]
    }
   ],
   "source": [
    "reader = PdfReader(path)\n",
    "doc = []\n",
    "for idx, page in enumerate(reader.pages):\n",
    "    doc.append(Document(text=page.extract_text(),\n",
    "                        metadata={'source': f'{paper.title}', 'page': f'{idx+1}'},\n",
    "                        excluded_llm_metadata_keys=[],\n",
    "                        excluded_embed_metadata_keys=['source', 'page']\n",
    "))\n",
    "\n",
    "print(f'Number of pages {len(doc)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Convert Documents into LlamaIndex Nodes\n",
    "We split our documents into 'chunks' to be embedded.  \n",
    "Each chunk is what LlamaIndex calls a **Node**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed the 75 pages into 124 nodes\n"
     ]
    }
   ],
   "source": [
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "parser = SimpleNodeParser.from_defaults(include_metadata = True, chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "nodes = parser.get_nodes_from_documents(doc)\n",
    "\n",
    "print(f'Parsed the {len(doc)} pages into {len(nodes)} nodes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import MetadataMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source: Language Models are Few-Shot Learners\n",
      "page: 31\n",
      "\n",
      "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\n",
      "split of our training distribution. Though there is some gap between training and validation performance, the gap grows\n",
      "only minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\n",
      "rather than overﬁtting.\n",
      "although models did perform moderately better on data that overlapped between training and testing, this did not\n",
      "signiﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\n",
      "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\n",
      "magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\n",
      "for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\n",
      "does not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with which it was\n",
      "deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as\n",
      "large as feared.\n",
      "We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap\n",
      "between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a\n",
      "bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t\n",
      "feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts\n",
      "results.\n",
      "For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, deﬁned roughly as\n",
      "examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when\n",
      "it is shorter than 13-grams). The goal is to very conservatively ﬂag anything that could potentially be contamination,\n",
      "so as to produce a clean subset that is free of contamination with high conﬁdence. The exact procedure is detailed in\n",
      "Appendix C.\n",
      "We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\n",
      "subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a\n",
      "signiﬁcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be\n",
      "inﬂating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a\n",
      "quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence\n",
      "that contamination level and performance difference are correlated. We conclude that either our conservative method\n",
      "substantially overestimated contamination or that contamination has little effect on performance.\n",
      "Below, we review in more detail the few speciﬁc cases where either (1) the model performs signiﬁcantly worse on\n",
      "the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference\n",
      "difﬁcult.\n"
     ]
    }
   ],
   "source": [
    "# This prints what the LLM sees\n",
    "print (nodes[50].get_content (metadata_mode=MetadataMode.LLM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Figure 4.1: GPT-3 Training Curves We measure model performance during training on a deduplicated validation\n",
      "split of our training distribution. Though there is some gap between training and validation performance, the gap grows\n",
      "only minimally with model size and training time, suggesting that most of the gap comes from a difference in difﬁculty\n",
      "rather than overﬁtting.\n",
      "although models did perform moderately better on data that overlapped between training and testing, this did not\n",
      "signiﬁcantly impact reported results due to the small fraction of data which was contaminated (often only a few percent).\n",
      "GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of\n",
      "magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential\n",
      "for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B\n",
      "does not overﬁt its training set by a signiﬁcant amount, measured relative to a held-out validation set with which it was\n",
      "deduplicated (Figure 4.1). Thus, we expect that contamination is likely to be frequent, but that its effects may not be as\n",
      "large as feared.\n",
      "We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap\n",
      "between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a\n",
      "bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn’t\n",
      "feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts\n",
      "results.\n",
      "For each benchmark, we produce a ‘clean’ version which removes all potentially leaked examples, deﬁned roughly as\n",
      "examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when\n",
      "it is shorter than 13-grams). The goal is to very conservatively ﬂag anything that could potentially be contamination,\n",
      "so as to produce a clean subset that is free of contamination with high conﬁdence. The exact procedure is detailed in\n",
      "Appendix C.\n",
      "We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean\n",
      "subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a\n",
      "signiﬁcant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be\n",
      "inﬂating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a\n",
      "quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence\n",
      "that contamination level and performance difference are correlated. We conclude that either our conservative method\n",
      "substantially overestimated contamination or that contamination has little effect on performance.\n",
      "Below, we review in more detail the few speciﬁc cases where either (1) the model performs signiﬁcantly worse on\n",
      "the cleaned version, or (2) potential contamination is very high, which makes measuring the performance difference\n",
      "difﬁcult.\n"
     ]
    }
   ],
   "source": [
    "# This prints what the embedding sees, you can see excluding source and page worked\n",
    "print (nodes[50].get_content (metadata_mode=MetadataMode.EMBED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Device:  \n",
    "If you are using a Mac or an Nvidia GPU and installed PyTorch correctly the below will use the correct device  \n",
    "Otherwise it will default to using the CPU\n",
    "\n",
    "For details on how to install PyTorch for CUDA see the [Get Started page](https://pytorch.org/get-started/locally/)  \n",
    "If you are not using CUDA with an Nvidia GPU you can uncomment the line below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch for Mac or Windows PC without Nvidia GPU \n",
    "# !pip install torch torchvision torchaudio \n",
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# Detect hardware acceleration device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available(): \n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Embedding Model:**  \n",
    "A good place to start when choosing and embedding model is the [MTEB English Leaderboard](https://huggingface.co/BAAI/bge-small-en)\n",
    "\n",
    "At time of writing, the [BAAI/bge-small-en-v1.5'model](https://huggingface.co/spaces/mteb/leaderboard) is the best small model according to the leaderboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "embed_model_name = 'BAAI/bge-small-en-v1.5'\n",
    "# Import embedding model from HuggingFace \n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embed_model_name,\n",
    "    device = device,\n",
    "    normalize='True', \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create and store the Vector DB\n",
    "* This will use the bge-small-en embeddings model to embed our chunked text into vectors\n",
    "* Then save those vectors into a ChromaDB named \"RAG_VectorDB\" \n",
    "\n",
    "**Note**: If a DB with that name already exists, it will append, otherwise it creates it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "531f45cb31754e27a0e7d51bd2461cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/124 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "db = chromadb.PersistentClient(path='./RAG_VectorDB')\n",
    "\n",
    "db_metadata = {\n",
    "    'embedding_used':embed_model_name,\n",
    "    'Included Papers':paper.title}\n",
    "chroma_collection = db.get_or_create_collection('arxiv_PDF_DB', metadata=db_metadata)\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model,\n",
    "                                                llm = None, # We will set the LLM when we open the DB\n",
    "                                                chunk_size=CHUNK_SIZE,\n",
    "                                                chunk_overlap=CHUNK_OVERLAP\n",
    "                                                )\n",
    "\n",
    "vector_store_index = VectorStoreIndex(nodes=nodes,\n",
    "                                    storage_context=storage_context, \n",
    "                                    service_context=service_context,\n",
    "                                    show_progress=True)\n",
    "\n",
    "print('Completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
