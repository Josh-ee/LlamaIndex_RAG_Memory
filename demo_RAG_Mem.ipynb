{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Utilizing the Vector Database with an Open Source LLM Model via LlamaCPP\n",
    "**Introduction:**  \n",
    "In this part, we will utilized the vectorDB we created in Part 1 to answer questions based on the documents inside.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n  Installing LlamaCPP can be a bit tricky\\nSee the full installation guide for GPU support at 'https://github.com/abetlen/llama-cpp-python'  \\n\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "  Installing LlamaCPP can be a bit tricky\n",
    "See the full installation guide for GPU support at 'https://github.com/abetlen/llama-cpp-python'  \n",
    "\"\"\"\n",
    "# Basic Windows install\n",
    "#!pip install llama-cpp-python  #basic install \n",
    "\n",
    "# MacOS: use the version below:\n",
    "# !pip install llama_cpp_python==0.2.12\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import VectorStoreIndex, ServiceContext\n",
    "from llama_index.vector_stores import ChromaVectorStore\n",
    "from llama_index.storage.storage_context import StorageContext\n",
    "import chromadb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "# Detect hardware acceleration device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_layers = 50\n",
    "elif torch.backends.mps.is_available():  # Assuming MPS backend exists\n",
    "    device = 'mps'\n",
    "    gpu_layers = 1\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    gpu_layers = 0\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load the Foundational LLM via LlamaCPP and ask a question\n",
    "Import the Foundation model form HuggingFace  \n",
    "* If this is your first time it can take up to 10 min\n",
    "* Currently using GGUF version of [Mistral-11B-OmniMix](https://huggingface.co/TheBloke/Mistral-11B-OmniMix-GGUF) with 4-bit Quantization \n",
    "* Hyperparams are set in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms import LlamaCPP\n",
    "\n",
    "model_url = 'https://huggingface.co/TheBloke/Mistral-11B-OmniMix-GGUF/resolve/main/mistral-11b-omnimix-bf16.Q4_K_M.gguf'\n",
    "\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    # We can pass the URL to a GGUF model to download it \n",
    "    model_url=model_url,\n",
    "    model_path=None,\n",
    "    temperature=0.0,\n",
    "    max_new_tokens=256,\n",
    "    context_window=3900,\n",
    "    generate_kwargs={},\n",
    "    model_kwargs={'n_gpu_layers': gpu_layers},\n",
    "    verbose=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Prompt:\n",
    "* The Default prompt is the prompt that the user's {question} is injected into"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt = \"\"\"\n",
    "    You are an AI assistant who is always happy and helpful.\n",
    "    Your answers must be appropriate for a 1st grade classroom, so no controversial topics or answers.\n",
    "    Please answer the following user question:\n",
    "    {question}\n",
    "\n",
    "    Please answer that question thinking step by step\n",
    "    Answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Logic Question\n",
    "No RAG Used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Prompt: \n",
      "    You are an AI assistant who is always happy and helpful.\n",
      "    Your answers must be appropriate for a 1st grade classroom, so no controversial topics or answers.\n",
      "    Please answer the following user question:\n",
      "    There are 3 birds in a nest, 2 fly away and then 3 eggs hatch, how many birds are there now?\n",
      "\n",
      "    Please answer that question thinking step by step\n",
      "    Answer:\n",
      "    \n",
      "\n",
      "Model Answer:\n",
      "1. First, we have to count the number of birds that were originally in the nest. We know that there were 3 birds in the nest.\n",
      "    2. Then, two of those birds flew away. So, now we have 3 - 2 = 1 bird left in the nest.\n",
      "    3. Finally, three eggs hatched and became baby birds. So, we add these new birds to the one that was already there: 1 + 3 = 4.\n",
      "    4. Therefore, now there are 4 birds in the nest."
     ]
    }
   ],
   "source": [
    "user_question = 'There are 3 birds in a nest, 2 fly away and then 3 eggs hatch, how many birds are there now?'\n",
    "\n",
    "full_question = default_prompt.format(question=user_question)\n",
    "print(f'Final Prompt: {full_question}\\n')\n",
    "print('Model Answer:')\n",
    "streaming_response = llm.stream_complete(full_question)\n",
    "for token in streaming_response:\n",
    "    print(token.delta, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use the LLM with RAG from VectorDB\n",
    "For RAG you need two models\n",
    "* A LLM model (loaded above)\n",
    "* A Embedding model, to embed the user question into a vector for the vector Data Base (DB) Search\n",
    "* Since we used the BGE small model in the creation of the DB, we **must** import that same embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor import SentenceEmbeddingOptimizer\n",
    "from llama_index.prompts  import PromptTemplate\n",
    "from llama_index.llms import ChatMessage, MessageRole\n",
    "from llama_index.chat_engine.condense_question import CondenseQuestionChatEngine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# Choose the same embedding model that used in the creation of the vector DB\n",
    "embed_model_name = 'BAAI/bge-small-en-v1.5'\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=embed_model_name,\n",
    "    device = device,\n",
    "    normalize='True' # since we normalized vectors when we created the DB we must do it here\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the RAG_VectorDB created in Part 1 from disk\n",
    "db = chromadb.PersistentClient(path='./RAG_VectorDB')\n",
    "\n",
    "chroma_collection = db.get_collection('arxiv_PDF_DB')\n",
    "\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Included Papers': 'Language Models are Few-Shot Learners',\n",
       " 'embedding_used': 'BAAI/bge-small-en-v1.5'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve our metadata\n",
    "chroma_collection.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "print(chroma_collection.metadata['embedding_used'])\n",
    "if embed_model_name != chroma_collection.metadata['embedding_used']:\n",
    "    raise Warning('Not using the same embedding model!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(embed_model=embed_model,\n",
    "                                               llm=llm,\n",
    "                                               )\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store,\n",
    "    service_context=service_context,\n",
    "    storage_context = storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.prompts import Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_prompt_with_context = (\n",
    "    \"\"\"\n",
    "    You are a \"PaperBot\", an AI assistant for answering questions about a arXiv paper. Assume all questions you receive are about this paper.\n",
    "    Please limit your answers to the information provided in the \"Context:\"\n",
    "\n",
    "    Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "    Context: {context_str}\n",
    "\n",
    "    Use that context to answer the following question about the paper.\n",
    "    Keep your answer concise.\n",
    "    Question: {query_str}\n",
    "    Answer: \"\"\")\n",
    "    \n",
    "qa_template = Prompt(default_prompt_with_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query with RAG\n",
    "Now we will ask a question and the following steps will happen:\n",
    "1. User question is turned into a vector \n",
    "2. That question vector is then compared to the vectors in our VectorDB\n",
    "3. The page_context of best \"k\" matches are returned as \"summaries\" \n",
    "4. We then pass the summaries and non vectorized user question into the default_prompt_with_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile_cutoff: a measure for using the top percentage of relevant sentences.\n",
    "query_engine = index.as_query_engine(streaming=True, similarity_top_k = 2, text_qa_template=qa_template,\n",
    "    node_postprocessors=[SentenceEmbeddingOptimizer(percentile_cutoff=0.2, embed_model=embed_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    The paper proved that language models can learn new tasks with only a few examples, without needing large amounts of labeled data.\n",
      "Source:\n",
      " Language Models are Few-Shot Learners, page: 51\n",
      " Language Models are Few-Shot Learners, page: 56\n"
     ]
    }
   ],
   "source": [
    "streaming_response = query_engine.query('What did the paper prove?')\n",
    "streaming_response.print_response_stream()\n",
    "\n",
    "print('\\nSource:')\n",
    "for source in streaming_response.metadata.values():\n",
    "    print(f' {source[\"source\"]}, page: {source[\"page\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can answer questions from our pdf.  \n",
    "However, the model has no memory of the conversation, as seen in the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Please provide a specific question related to the given context."
     ]
    }
   ],
   "source": [
    "# Lacks Conversational Memory\n",
    "streaming_response = query_engine.query('What did I just ask you?')\n",
    "streaming_response.print_response_stream() # Will hallucinate the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Conversational Memory with RAG and Sources\n",
    "Order of operations depends on when the question is asked.\n",
    "* If it is the first time the user asks a question. Then their exact question is put into the default prompt\n",
    "\n",
    "* For every prompt after that first question the procedure is as follows:\n",
    "    1. Use the condense_question_prompt to input chat history and the users followup question to generate a Standalone question\n",
    "        * This Standalone question rephrases the users question in context of the chat history\n",
    "    2. Pass the Standalone question into the default prompt along with the RAG data\n",
    "    \n",
    "#### Key Takeaway: For follow up questions the LLM is used twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = PromptTemplate(\"\"\"\\\n",
    "Your objective is to take in the USER QUESTION and add additional context (especially Nouns) from the CHAT HISTORY\n",
    "rephrase the user question to be a Standalone Question by combining it with the relevant CHAT HISTORY.\n",
    "The question is always about the arXiv paper, do not modify acronyms.\n",
    "\n",
    "<CHAT HISTORY>\n",
    "{chat_history}\n",
    "                               \n",
    "<USER QUESTION>\n",
    "{question}\n",
    "\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\")\n",
    "\n",
    "# custom_chat_history: list of ChatMessage objects\n",
    "custom_chat_history = []\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    embed_model=embed_model,\n",
    "    service_context = service_context,\n",
    "    condense_question_prompt=custom_prompt,\n",
    "    chat_history=custom_chat_history,\n",
    "    verbose=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " They described zero-shot as a setting where no demonstrations are provided, and the model only receives a natural language instruction describing the task.\n",
      "Source:\n",
      " Language Models are Few-Shot Learners, page: 7\n",
      " Language Models are Few-Shot Learners, page: 4\n"
     ]
    }
   ],
   "source": [
    "# First question, just ask query_engine directly \n",
    "chat_engine.reset()\n",
    "question ='How did they describe zero-shot?'\n",
    "\n",
    "streaming_response = chat_engine._query_engine.query(question)\n",
    "# streaming_response = query_engine.query(question)\n",
    "streaming_response.print_response_stream()\n",
    "\n",
    "print('\\nSource:')\n",
    "for v in streaming_response.metadata.values():\n",
    "    print(f' {v[\"source\"]}, page: {v[\"page\"]}')\n",
    "\n",
    "\n",
    "# Need to manually append history on first question since we used query_engine instead of chat_engine for first question\n",
    "chat_engine.chat_history.append(\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content = question\n",
    "    )\n",
    " \n",
    ")\n",
    "chat_engine.chat_history.append(\n",
    "    ChatMessage(\n",
    "    role=MessageRole.ASSISTANT,\n",
    "    content = streaming_response.response_txt\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatMessage(role=<MessageRole.USER: 'user'>, content='How did they describe zero-shot?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=' They described zero-shot as a setting where no demonstrations are provided, and the model only receives a natural language instruction describing the task.', additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_engine.chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: Can you please elaborate on how zero-shot learning differs from few-shot learning in the context of the given paper?\n",
      " In the context of the paper, zero-shot learning and few-shot learning differ primarily in the amount of demonstration data provided to the model during inference.\n",
      "    In zero-shot learning, no demonstrations are given, and the model only has access to a natural language instruction describing the task. This method is challenging but potentially more robust and generalizable since it avoids spurious correlations that may arise from specific examples.\n",
      "    On the other hand, in few-shot learning, the model is provided with a small set of demonstrations (usually around 10-20) to learn from. This method strikes a balance between performance and sample efficiency, allowing the model to learn new tasks without requiring large amounts of data.\n",
      "Source:\n",
      " Language Models are Few-Shot Learners, page: 7\n",
      " Language Models are Few-Shot Learners, page: 4\n"
     ]
    }
   ],
   "source": [
    "streaming_response = chat_engine.stream_chat('How does that compare to Few-Shot?')\n",
    "streaming_response.print_response_stream()\n",
    "\n",
    "print('\\nSource:')\n",
    "for node in streaming_response.sources[0].raw_output.source_nodes:\n",
    "    print(f' {node.metadata[\"source\"]}, page: {node.metadata[\"page\"]}')\n",
    "    #print(node.score) # similarity score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ChatMessage(role=<MessageRole.USER: 'user'>, content='How did they describe zero-shot?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=' They described zero-shot as a setting where no demonstrations are provided, and the model only receives a natural language instruction describing the task.', additional_kwargs={}), ChatMessage(role=<MessageRole.USER: 'user'>, content='How does that compare to Few-Shot?', additional_kwargs={}), ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content=' In the context of the paper, zero-shot learning and few-shot learning differ primarily in the amount of demonstration data provided to the model during inference.\\n    In zero-shot learning, no demonstrations are given, and the model only has access to a natural language instruction describing the task. This method is challenging but potentially more robust and generalizable since it avoids spurious correlations that may arise from specific examples.\\n    On the other hand, in few-shot learning, the model is provided with a small set of demonstrations (usually around 10-20) to learn from. This method strikes a balance between performance and sample efficiency, allowing the model to learn new tasks without requiring large amounts of data.', additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(chat_engine.chat_history)\n",
    "chat_engine.reset() # clears chat history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_index",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
